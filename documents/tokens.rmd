---
title: "Rによる日本語のテキスト分析：tokensの作成・操作・分析"
author: "渡辺耕平 (K.Watanabe1@lse.ac.uk)"
date: "05 December 2017"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = "~/Documents/IJTA/")
knitr::opts_chunk$set(collapse = FALSE)
```
# tokens
**tokens**は文書を語に分割した状態で効率的に処理する**quanteda**の独自オブジェクト。**tokens**の作成・操作を行う関数は`tokens_*`と命名されている。トークン化（tokenization）とは、自然言語を機械が扱える形に変換する処理を指す。

## 作成
`tokens()`は[**ICU**](http://site.icu-project.org)に内蔵された辞書に基づき、日本語(および中国語)のトークン化を行うたため、MecabやChasenなどの形態素解析ツールを必要としない。テキストがあらかじめ半角スペースで分かち書きされている場合は、`what = "fastestword"`とする。

```{r, message=FALSE}
require(quanteda) # パッケージの読み込み
```
```{r creation}
# 分かち書きされている場合
load('data/data_corpus_asahi_2016_seg.rda') # Mecabで分かち書き済み
toks <- tokens(data_corpus_asahi_2016_seg, what = "fastestword", remove_punct = FALSE)

# 分かち書きされていない場合
load('data/data_corpus_asahi_2016.rda') 
toks <- tokens(data_corpus_asahi_2016, remove_punct = FALSE)
```
**quanteda**は**Mecab**を呼び出す関数を含んでいないが、**RMeCab**を用いると容易に[Mecabによる分かち書き](../extra/mecab.R)を行える

## 操作
**tokens**は語の位置を保持するため`padding = TRUE`とすると、削除した語が空文字となり、元々の語の距離を維持することができる。
```{r manipulation}
# 文字以外を削除
toks <- tokens_select(toks, '^[０-９ぁ-んァ-ヶー一-龠]+$', valuetype = 'regex', padding = TRUE)
# ひらがなを削除
toks <- tokens_remove(toks, '^[ぁ-ん]+$', valuetype = 'regex', padding = TRUE)

# KWICで用例を確認
head(kwic(toks, "トランプ"), 20)
```
```{r eval=FALSE}
# KWICをより見やすく別のウィンドウで表示
View(head(kwic(toks, "トランプ"), 100))

```{r ngram}
# Nグラムの生成
toks_ngram <- tokens_ngrams(toks, n = 2)
head(toks_ngram[[1]], 20)
```

## 分析
### 辞書分析
```{r analysis1}
# 地理的辞書の読み込み (Watanabe 2017)
dict <- dictionary(file = 'extra/watanabe_country.yml')
head(dict['ASIA'])

# 国コードでtokensを作成
toks_country <- tokens_lookup(toks, dict, levels = 3) 
head(toks_country)

# 集計
mx_country <- dfm(toks_country)
county_top <- topfeatures(mx_country)
```

```{r plot, echo=TRUE, results='asis', fig.height=4, fig.width=7, dpi=100}
barplot(county_top)
```

## 共起語分析
連続的共起語を分析する際は、どのような種類の語の連続を抽出するのかを考慮し、句読点を削除する際も語の間の距離が維持されている必要がある。このために、上記の例では、`tokens()`において`remove_punct = FALSE`とし、`tokens_remove()`おいては`padding = TRUE`としてある。
```{r analysis2, cache=TRUE}
# 連続的共起語の抽出
seqs <- textstat_collocations(toks, 'lambda', min_count = 10)
head(seqs, 20)

# 共起語の結合
toks_comp <- tokens_compound(toks, seqs[seqs$p < 0.01, 'collocation'], concatenator = '')

head(kwic(toks, "トランプ*", window = 10)) # 結合前
head(kwic(toks_comp, "トランプ*", window = 10)) # 結合後
```

